<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chapter 12 | CSU MSMB Group Study</title>
    <link>/categories/chapter-12/</link>
      <atom:link href="/categories/chapter-12/index.xml" rel="self" type="application/rss+xml" />
    <description>Chapter 12</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 04 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Chapter 12</title>
      <link>/categories/chapter-12/</link>
    </image>
    
    <item>
      <title>Vocabulary for Chapter 12</title>
      <link>/post/vocabulary-for-chapter-12/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/vocabulary-for-chapter-12/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Chapter 12 covers supervised learning and the statistics of predicting categorical variables. Also discussed are the issues of overfitting and generalizability and how to “train” statistical models.&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
predictors
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
whether a property of interest (disease type, cell type, the prognosis of a patient) can be “predicted”, given one or more of these properties
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
overfitting
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future obervations reliably
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
generalization
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
refers to how well the concepts learned by a machine learning model apply to specific examples not seen by the model when it was learning.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
statistical learning
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
framework for machine learning drawing from the fields of statistics and functional analysis. Deals with the problem of finding a predictive function based on data.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
objective response (in context of supervised learning)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a measurable response
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
kernel methods
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
do not need features at all; instead, use distances or measures of similarity between objects.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
regression
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
statistical method that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
classification
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
analysis for categorical response variables in the framework of supervised learning
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
linear discriminant analysis (LDA)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a common technique used supervised learning classification problems; a dimensionality reduction technique used as a preprocessing step in Machine Learning and pattern classification applications.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
misclassification rate (MCR)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in regards to statistical learning, the fraction of times the prediction is wrong
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
leave-one out cross-validation
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
K-fold cross validation taken to its logical extreme, with K equal to N, the number of data points in the set
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
k-fold cross-validation
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
observations are repeatedly split into a training set of size around n(k-1)/k and a test set of size of around n/k
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
curse of dimensionality
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
refers to the fact that high-dimensional spaces are very hard, if not impossible, to sample thoroughly
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
confusion table
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the field of machine learning and specifically the problem of statistical classification, a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
sensitvity
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
true positivity rate or recall, related to the misclassification of healthy as ill
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
specificity
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
true negative rate, the probability of misclassification of ill as healthy
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
receiver operating characteristic (ROC)/precision recall curves
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
Jaccard index
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a statistic used in quantifying the similarities between sample sets, which is formally defined as the size of the intersection between two sets divided by the size of the union of the sets
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
mean-squared error (MSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the average squared error
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
risk function/cost function/objective function
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the expected value of a loss of function
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
bias
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
measures how different the average of all the different estimates is from the truth
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
variance
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
how much an individual estimate might scatter from the average value
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
penalization
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a tool to actively control and exploit the variance-bias tradeoff
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
regularization
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
methods used to to ensure stable estimates
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
logistic regression
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
classification algorithm used to assign observations to a discrete set of classes
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
penalty function
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a term added to the objective function that consists of a penalty parameter multiplied by a measure of violation of the contstraints
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
ridge regression
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients, shrinks coefficients and helps reduce model complexity and mutli-collinearity
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
lasso (in the context of statistical regression modeling)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
least absolute shrinkage and selection operator, helps in reducing over-fitting and in feature selection
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
elastic net
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the fitting of linear or logistic regression models, a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
ExperimentalHub (in the context of Bioconductor)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
provides a central location where curated data from experiments, publications or training courses can be accessed
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
kingdom
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the second highest taxonomic rank, just below domain
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
phylum
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a level of classification of taxonomic rank below kingdom and above class
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
species
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the basic unit of classification and a taxonomic rank of an organism, as well as unit of biodiversity
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
diagnostic plot
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
plots that help you evaluate the assumption of a regression analysis
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
tuning parameters
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
sometimes called a penalty parameter, controls the strength of the penalty term in ridge and lasso regression. Basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
p-value hacking
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
manipulation of the data until finding a statistic that yields a desired result
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
workflow (in the context of a computational analysis)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
chains software tools together in a series of steps that operate on data
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
scale invariance
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a feature of objects or laws that do not change if scales, length, energy, or other variables, are multiplied by a common factor, and thus represent universality
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The vocabulary words for Chapter 12 are:&lt;/p&gt;
&lt;div id=&#34;sources-consulted-or-cited&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sources consulted or cited&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Some of the definitons above are based in part or whole on listed definitions in the following source:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Holmes and Huber, 2019. &lt;em&gt;Modern Statistics for Modern Biology.&lt;/em&gt; Cambridge University Press,
Cambridge, United Kingdom.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/&lt;/a&gt;
&lt;a href=&#34;https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/&#34; class=&#34;uri&#34;&gt;https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/&lt;/a&gt;
&lt;a href=&#34;https://.statisticshowto.com&#34; class=&#34;uri&#34;&gt;https://.statisticshowto.com&lt;/a&gt;
&lt;a href=&#34;https://www.cs.cmu.edu/~schneide/tut5/node42.html&#34; class=&#34;uri&#34;&gt;https://www.cs.cmu.edu/~schneide/tut5/node42.html&lt;/a&gt;
&lt;a href=&#34;https://towardsdatascience.com&#34; class=&#34;uri&#34;&gt;https://towardsdatascience.com&lt;/a&gt;
&lt;a href=&#34;https://bioconductor.org&#34; class=&#34;uri&#34;&gt;https://bioconductor.org&lt;/a&gt;
&lt;a href=&#34;https://pfern.gtihub.io&#34; class=&#34;uri&#34;&gt;https://pfern.gtihub.io&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;practice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Practice&lt;/h3&gt;
&lt;iframe src=&#34;https://quizlet.com/506392624/match/embed?i=takib&amp;amp;x=1jj1&#34; height=&#34;500&#34; width=&#34;100%&#34; style=&#34;border:0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
