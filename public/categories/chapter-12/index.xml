<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chapter 12 | CSU MSMB Group Study</title>
    <link>/categories/chapter-12/</link>
      <atom:link href="/categories/chapter-12/index.xml" rel="self" type="application/rss+xml" />
    <description>Chapter 12</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 12 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Chapter 12</title>
      <link>/categories/chapter-12/</link>
    </image>
    
    <item>
      <title>Exercise Solution for Chapter 12</title>
      <link>/post/exercise-solution-for-chapter-12/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-12/</guid>
      <description>


&lt;div id=&#34;exercise-12.2-from-modern-statistics-for-modern-biologists&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercise 12.2 from Modern Statistics for Modern Biologists&lt;/h2&gt;
&lt;p&gt;Use glmnet for a prediction of a continous variable, i.e., for regression. Use the prostate cancer data from Chapter 3 of (Hastie, Tibshirani, and Friedman 2008). The data are available in the CRAN package ElemStatLearn. Explore the effects of using ridge versus lasso penalty.&lt;/p&gt;
&lt;p&gt;Here are the packages that need to be installed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(glmnet)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-for-the-exercise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data for the exercise&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;ElemStatPackage&lt;/code&gt; has been orphaned and isn’t on CRAN anymore. However, it’s up on GitHub, so I grabbed the data file you’ll need from there. You can download it yourself at: &lt;a href=&#34;https://github.com/cran/ElemStatLearn/blob/master/data/prostate.RData&#34; class=&#34;uri&#34;&gt;https://github.com/cran/ElemStatLearn/blob/master/data/prostate.RData&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;./example_datasets/prostate.RData&amp;quot;)

prostate %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa
## 1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829
## 2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189
## 3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189
## 4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189
## 5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636
## 6 -1.0498221 3.228826  50 -1.386294   0 -1.386294       6     0  0.7654678
##   train
## 1  TRUE
## 2  TRUE
## 3  TRUE
## 4  TRUE
## 5  TRUE
## 6  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a description of the data, from the archived help files:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Data to examine the correlation between the level of prostate-specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here’s what the variables mean:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lcavol&lt;/code&gt;: log cancer volume&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lweight&lt;/code&gt;: log prostate weight&lt;/li&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt;: in years&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lbph&lt;/code&gt;: log of the amount of benign prostatic hyperplasia&lt;/li&gt;
&lt;li&gt;&lt;code&gt;svi&lt;/code&gt;: seminal vesicle invasion&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lcp&lt;/code&gt;: log of capsular penetration&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gleason&lt;/code&gt;: a numeric vector with the Gleason score&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pgg45&lt;/code&gt;: percent of Gleason score 4 or 5&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lpsa&lt;/code&gt;: response (the thing you are trying to predict), the level of prostate-specific antigen&lt;/li&gt;
&lt;li&gt;&lt;code&gt;train&lt;/code&gt;: a logical vector, of whether the data was to be part of the training dataset (TRUE) or the testing one (FALSE)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, you’re trying to predict the values of &lt;code&gt;lpsa&lt;/code&gt; based on the variables &lt;code&gt;lcavol&lt;/code&gt; through &lt;code&gt;pgg45&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We will first split the data into testing and training data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prostate_train &amp;lt;- prostate %&amp;gt;%
  filter(train == TRUE)

prostate_test &amp;lt;- prostate %&amp;gt;%
  filter(train == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 67 samples in the training set and 30 samples in the testing set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-generaltized-linear-model-glmnet-with-lasso-and-ridge-penalties&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit generaltized linear model (glmnet) with Lasso and Ridge penalties&lt;/h2&gt;
&lt;p&gt;Based on the glmnet package, when alpha = 1 the lasso penalty is used, if alpha = 0, then ridge penalty is used. A great resource for the glmnet package can be found here: &lt;a href=&#34;https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html&#34; class=&#34;uri&#34;&gt;https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here, we use a matrix of all of the predictors (&lt;code&gt;x&lt;/code&gt;) to try to predict the &lt;code&gt;lpsa&lt;/code&gt; column (&lt;code&gt;y&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Lasso 
lasso_glmnet &amp;lt;- glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), 
                    y = prostate_train %&amp;gt;% pull(lpsa), 
                    family = &amp;quot;gaussian&amp;quot;, alpha = 1)

plot(lasso_glmnet, label = TRUE)
title(&amp;quot;Lasso Penalty&amp;quot;, line = -2.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Ridge
ridge_glmnet &amp;lt;- glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), 
                    y = prostate_train %&amp;gt;% pull(lpsa), 
                    family = &amp;quot;gaussian&amp;quot;, alpha = 0)

plot(ridge_glmnet, label = TRUE)
title(&amp;quot;Ridge Penalty&amp;quot;, line = -1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the plots show the coefficients as L1- norm increases. The top axis shows the number of nonzero coefficients correspoding to the lamba at the current L1 norm. From these plots we can see that the variables 1 (log cancer volume), 2 (log prostate weight), and 5 (seminal vesicle invasion) are good predictors for the level of prostate-specific antigen (lpsa response).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross Validation&lt;/h2&gt;
&lt;p&gt;We then want to perform cross-validation on the dataset. We use the cv.glmnet function to do this. Again we input a matrix of all of the predictors (&lt;code&gt;x&lt;/code&gt;) to look at the lpsa response (&lt;code&gt;y&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2)

# Lasso
cvglmnet_lasso &amp;lt;- cv.glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), 
                    y = prostate_train %&amp;gt;% pull(lpsa), 
                    family = &amp;quot;gaussian&amp;quot;, alpha = 0)
cvglmnet_lasso&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  cv.glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;%      as.matrix(), y = prostate_train %&amp;gt;% pull(lpsa), family = &amp;quot;gaussian&amp;quot;,      alpha = 0) 
## 
## Measure: Mean-Squared Error 
## 
##     Lambda Measure      SE Nonzero
## min 0.0879  0.5939 0.09857       8
## 1se 0.9873  0.6873 0.08098       8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(cvglmnet_lasso)
title(&amp;quot;Lasso Cross Validation&amp;quot;, line = -1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Ridge
cvglmnet_ridge &amp;lt;- cv.glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), 
                    y = prostate_train %&amp;gt;% pull(lpsa), 
                    family = &amp;quot;gaussian&amp;quot;, alpha = 1)
cvglmnet_ridge&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  cv.glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;%      as.matrix(), y = prostate_train %&amp;gt;% pull(lpsa), family = &amp;quot;gaussian&amp;quot;,      alpha = 1) 
## 
## Measure: Mean-Squared Error 
## 
##      Lambda Measure      SE Nonzero
## min 0.01336  0.5753 0.07244       7
## 1se 0.13673  0.6408 0.09222       5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(cvglmnet_ridge)
title(&amp;quot;Ridge Cross Validation&amp;quot;, line = -1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the data output, &lt;code&gt;1se&lt;/code&gt; means the data point what is within 2 standard error of the minimum lambda (&lt;code&gt;min&lt;/code&gt;). This it the value that the model suggests that we use (indicated by the 2nd dotted line on the plots.) The &lt;code&gt;1se Measure&lt;/code&gt; is similar to the mean squared error. If the measure is small, then the model is better. When comparing the &lt;code&gt;Measure&lt;/code&gt; of the &lt;code&gt;1se&lt;/code&gt; between the two penalties, we can see that the Ridge Penalty has a smaller 1se Measure, showing that it performs better.&lt;/p&gt;
&lt;p&gt;The Nonzero column describes the nonzero coefficients, or the number of predictors that are important in the particular model. There were a total of 8 predictors as the input. The Lasso penalty shows that all 8 predictors are important in building the model, but the Ridge penalty only uses 5 predictors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lasso Prediction&lt;/h2&gt;
&lt;p&gt;As we used the training data to build the model, we can then test the generalized linear model with the lasso penalty on the testing data.&lt;/p&gt;
&lt;p&gt;We start by using the &lt;code&gt;predict&lt;/code&gt; function to use the model to predict the lpsa on the testing data. We can then see the correlation between the predicted values and actual values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s0 &amp;lt;- cvglmnet_lasso$lambda.1se 

lasso_predict &amp;lt;- predict(cvglmnet_lasso, newx = prostate_test%&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), s = s0)

# create a data frame of the actual lpsa values and the predicted lpsa values
actual_lasso_predict_df &amp;lt;- data.frame(prediction = as.vector(lasso_predict), actual = prostate_test$lpsa)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then see how correlated the prediction and real data are using the &lt;code&gt;cor&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# look at the correlation of the prediction and real data
cor(actual_lasso_predict_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            prediction    actual
## prediction  1.0000000 0.7277086
## actual      0.7277086 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output shows that the actual and predicted values are 72% correlated.&lt;/p&gt;
&lt;p&gt;We can then fit a linear line to the prediction and actual data and look at the r&lt;sup&gt;2&lt;/sup&gt; value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(actual_lasso_predict_df) %&amp;gt;%
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = actual_lasso_predict_df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.80115 -0.24539 -0.05877  0.24141  0.93424 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.40143    0.20585   6.808 2.14e-07 ***
## actual       0.42246    0.07525   5.614 5.19e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.4223 on 28 degrees of freedom
## Multiple R-squared:  0.5296, Adjusted R-squared:  0.5128 
## F-statistic: 31.52 on 1 and 28 DF,  p-value: 5.194e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The adjusted r&lt;sup&gt;2&lt;/sup&gt; value = 0.513.&lt;/p&gt;
&lt;p&gt;Finally, we can plot the actual vs. predicted values on a scatter plot. If the actual and predicted values match up exactly, they would sit on the y = x line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(actual_lasso_predict_df, aes(x = actual, y = prediction)) +
  geom_point(color = &amp;quot;#00B0F6&amp;quot;, size = 2) +
  geom_abline(slope=1, intercept=0)+
  ggtitle(&amp;quot;Lasso Prediction&amp;quot;) +
  theme_light()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ridge-prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ridge Prediction&lt;/h2&gt;
&lt;p&gt;We can then perform the same functions using the generalized linear model with the ridge penalty to test on the testing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s0 &amp;lt;- cvglmnet_ridge$lambda.1se 

ridge_predict &amp;lt;- predict(cvglmnet_ridge, newx = prostate_test%&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), s = s0)

# create a data frame of the predicted values and actual values
actual_ridge_predict_df &amp;lt;- data.frame(prediction = as.vector(ridge_predict), actual = prostate_test$lpsa) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then see how correlated the prediction and real data are using the &lt;code&gt;cor&lt;/code&gt; function again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(actual_ridge_predict_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            prediction    actual
## prediction  1.0000000 0.7765403
## actual      0.7765403 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ridge prediction and acutal values are 77% correlated.&lt;/p&gt;
&lt;p&gt;We can then fit a linear line to the prediction and actual data and look at the r&lt;sup&gt;2&lt;/sup&gt; value&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(actual_ridge_predict_df) %&amp;gt;%
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = actual_ridge_predict_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.8438 -0.2155 -0.0225  0.2252  0.6732 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.33258    0.19271   6.915 1.62e-07 ***
## actual       0.45941    0.07044   6.522 4.55e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3953 on 28 degrees of freedom
## Multiple R-squared:  0.603,  Adjusted R-squared:  0.5888 
## F-statistic: 42.53 on 1 and 28 DF,  p-value: 4.551e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The adjusted r&lt;sup&gt;2&lt;/sup&gt; value = 0.589&lt;/p&gt;
&lt;p&gt;Finally, we can plot the actual vs. predicted values on a scatter plot. If the actual and predicted values matched up exactly, they would sit on the y = x line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(actual_ridge_predict_df, aes(x = actual, y = prediction)) +
  geom_point(color = &amp;quot;#FF62BC&amp;quot;, size = 2) +
  geom_abline(slope=1, intercept=0) +
  ggtitle(&amp;quot;Ridge Prediction&amp;quot;) +
  theme_light()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Comparing the Lasso and Ridge Penalty, based on the cross-validation, the ridge penalty had a smaller 1se Measure, showing that it performs better. When looking at the actual predictions on the testing data, the ridge penalty had a higher correlation between the predicted and acutal values (77%) compared to the lasso penatly correlation (72%). Further, when fitting a linear model to the actual and predicted values, the r&lt;sup&gt;2&lt;/sup&gt; values were highter for the ridge penalty (0.589) versus the lasso penalty (0.513). In conclusion, the &lt;strong&gt;ridge penalty performed better on this particular data set.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Vocabulary for Chapter 12</title>
      <link>/post/vocabulary-for-chapter-12/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/vocabulary-for-chapter-12/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Chapter 12 covers supervised learning and the statistics of predicting categorical variables. Also discussed are the issues of overfitting and generalizability and how to “train” statistical models.&lt;/p&gt;
&lt;p&gt;The vocabulary words for Chapter 12 are:&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
predictors
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
characteristics measured for an observation that may be useful in predicting the target variable
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
overfitting
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future obervations reliably
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
generalization
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
refers to how well the concepts learned by a machine learning model apply to specific examples not seen by the model when it was learning
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
statistical learning
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
framework for machine learning drawing from the fields of statistics and functional analysis. Deals with the problem of finding a predictive function based on data
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
objective response
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the context of supervised learning, a measurable response
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
kernel methods
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). These use kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
regression
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
statistical method that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
classification
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the process of grouping observations in a dataset by their similarities in terms of measured characteristics
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
linear discriminant analysis (LDA)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a common technique used both for supervised learning classification and as a pre-processing dimension reduction step that finds a linear combination of features to help in classification
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
misclassification rate (MCR)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in regards to statistical learning, the fraction of times the prediction is wrong, specifically when relating to classification of models
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
leave-one out cross-validation
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
k-fold cross validation taken to its logical extreme, with K equal to N, the number of data points in the set
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
k-fold cross-validation
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a technique where observations are repeatedly split into a training set of size around n(k-1)/k and a test set of size of around n/k. Mainly used in prediction when one wants to estimate how accurately a predictive model will perform in practice
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
curse of dimensionality
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
refers to the fact that high-dimensional spaces are very hard, if not impossible, to sample thoroughly, because data in any particular region becomes very sparse as dimensions increase
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
confusion table
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the field of machine learning and specifically the problem of statistical classification, a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one. By counting the number of observations truly within each class versus the number predicted by the model to be in each class
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
sensitvity
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
true positivity rate or recall, measures the proportion of actual positives that are correctly identified as such
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
specificity
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
true negative rate, the probability, measures the proportion of actual negatives that are correctly identified as negative
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
receiver operating characteristic(ROC)/precision recall curve
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
Jaccard index
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a statistic used in quantifying the similarities between sample sets, which is formally defined as the size of the intersection between two sets divided by the size of the union of the sets
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
mean-squared error (MSE)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the average squared error
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
risk function/cost function/objective function
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the function that you optimize during the training of a predictive model (e.g., the maximum likelihood function for a classic regression model)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
bias
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a measure of how different the average of all the different estimates is from the truth
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
variance
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
how much an individual estimate might scatter from the average value
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
penalization
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a tool to actively control and exploit the variance-bias tradeoff
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
regularization
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a method used to to ensure stable estimates by helping to prevent overfitting of the model to the training data
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
logistic regression
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a statistical model that in its basic form uses a logistic function to model a binary dependent variable. A binary logistic model has a dependent variable with two possible values (e.g, healthy/sick) which are represented by indicator variables (0,1)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
penalty function
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a term added to the objective function that consists of a penalty parameter multiplied by a measure of violation of the constraints
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
ridge regression
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a method of regression in which the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients. Doing this shrinks coefficients and helps reduce model complexity and mutli-collinearity
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
lasso
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the context of statistical regression modeling, Least Absolute Shrinkage and Selection Operator that is used in one type of regression modeling to reduce over-fitting and select useful features of hte data for predicting the outcome
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
elastic net
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the fitting of linear or logistic regression models, a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
ExperimentalHub
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the context of Bioconductor, provides a central location where curated data from experiments, publications or training courses can be accessed
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
kingdom
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the second highest taxonomic rank, just below domain
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
phylum
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a level of classification of taxonomic rank below kingdom and above class
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
species
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the basic unit of classification and a taxonomic rank of an organism, as well as unit of biodiversity
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
diagnostic plots
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
statistical influence plots that help to visualize how well a model fits the data (e.g. Normal Q-Q, Residuals vs Fitted)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
tuning parameters
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
parameters that control the strength of the penalty term in certain types of regression algorithms (e.g., ridge and lasso regression), controlling the amount of shrinkage (where parameter estimates are shrunk towards a central point, like the mean) when fitting the mode
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
p-value hacking
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
manipulation of the data until finding a statistic that yields a desired result
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
workflow
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the context of a computational analysis, the chaining of software tools together in a series of steps that operate on data
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
scale invariance
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a feature of objects or laws that do not change if scales, length, energy, or other variables, are multiplied by a common factor, and thus represent universality
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;sources-consulted-or-cited&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sources consulted or cited&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Some of the definitions above are based in part or whole on listed definitions in the following source:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Holmes and Huber, 2019. &lt;em&gt;Modern Statistics for Modern Biology.&lt;/em&gt; Cambridge University Press,
Cambridge, United Kingdom.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/&#34; class=&#34;uri&#34;&gt;https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://.statisticshowto.com&#34; class=&#34;uri&#34;&gt;https://.statisticshowto.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~schneide/tut5/node42.html&#34; class=&#34;uri&#34;&gt;https://www.cs.cmu.edu/~schneide/tut5/node42.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com&#34; class=&#34;uri&#34;&gt;https://towardsdatascience.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bioconductor.org&#34; class=&#34;uri&#34;&gt;https://bioconductor.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pfern.gtihub.io&#34; class=&#34;uri&#34;&gt;https://pfern.gtihub.io&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;practice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Practice&lt;/h3&gt;
&lt;iframe src=&#34;https://quizlet.com/506486027/flashcards/embed?i=takib&amp;amp;x=1jj1&#34; height=&#34;500&#34; width=&#34;100%&#34; style=&#34;border:0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
