<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CSU MSMB Group Study</title>
    <link>/authors/amy-fox/</link>
      <atom:link href="/authors/amy-fox/index.xml" rel="self" type="application/rss+xml" />
    <description>CSU MSMB Group Study</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 12 May 2020 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Exercise Solution for Chapter 12</title>
      <link>/post/exercise-solution-for-chapter-12/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/exercise-solution-for-chapter-12/</guid>
      <description>


&lt;div id=&#34;exercise-12.2-from-modern-statistics-for-modern-biologists&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercise 12.2 from Modern Statistics for Modern Biologists&lt;/h2&gt;
&lt;p&gt;Use glmnet for a prediction of a continous variable, i.e., for regression. Use the prostate cancer data from Chapter 3 of (Hastie, Tibshirani, and Friedman 2008). The data are available in the CRAN package ElemStatLearn. Explore the effects of using ridge versus lasso penalty.&lt;/p&gt;
&lt;p&gt;Here are the packages that need to be installed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(glmnet)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-for-the-exercise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data for the exercise&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;ElemStatPackage&lt;/code&gt; has been orphaned and isn’t on CRAN anymore. However, it’s up on GitHub, so I grabbed the data file you’ll need from there. You can download it yourself at: &lt;a href=&#34;https://github.com/cran/ElemStatLearn/blob/master/data/prostate.RData&#34; class=&#34;uri&#34;&gt;https://github.com/cran/ElemStatLearn/blob/master/data/prostate.RData&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;./example_datasets/prostate.RData&amp;quot;)

prostate %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa
## 1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829
## 2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189
## 3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189
## 4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189
## 5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636
## 6 -1.0498221 3.228826  50 -1.386294   0 -1.386294       6     0  0.7654678
##   train
## 1  TRUE
## 2  TRUE
## 3  TRUE
## 4  TRUE
## 5  TRUE
## 6  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a description of the data, from the archived help files:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Data to examine the correlation between the level of prostate-specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here’s what the variables mean:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lcavol&lt;/code&gt;: log cancer volume&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lweight&lt;/code&gt;: log prostate weight&lt;/li&gt;
&lt;li&gt;&lt;code&gt;age&lt;/code&gt;: in years&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lbph&lt;/code&gt;: log of the amount of benign prostatic hyperplasia&lt;/li&gt;
&lt;li&gt;&lt;code&gt;svi&lt;/code&gt;: seminal vesicle invasion&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lcp&lt;/code&gt;: log of capsular penetration&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gleason&lt;/code&gt;: a numeric vector with the Gleason score&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pgg45&lt;/code&gt;: percent of Gleason score 4 or 5&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lpsa&lt;/code&gt;: response (the thing you are trying to predict), the level of prostate-specific antigen&lt;/li&gt;
&lt;li&gt;&lt;code&gt;train&lt;/code&gt;: a logical vector, of whether the data was to be part of the training dataset (TRUE) or the testing one (FALSE)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, you’re trying to predict the values of &lt;code&gt;lpsa&lt;/code&gt; based on the variables &lt;code&gt;lcavol&lt;/code&gt; through &lt;code&gt;pgg45&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We will first split the data into testing and training data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prostate_train &amp;lt;- prostate %&amp;gt;%
  filter(train == TRUE)

prostate_test &amp;lt;- prostate %&amp;gt;%
  filter(train == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 67 samples in the training set and 30 samples in the testing set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-generaltized-linear-model-glmnet-with-lasso-and-ridge-penalties&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit generaltized linear model (glmnet) with Lasso and Ridge penalties&lt;/h2&gt;
&lt;p&gt;Based on the glmnet package, when alpha = 1 the lasso penalty is used, if alpha = 0, then ridge penalty is used. A great resource for the glmnet package can be found here: &lt;a href=&#34;https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html&#34; class=&#34;uri&#34;&gt;https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here, we use a matrix of all of the predictors (&lt;code&gt;x&lt;/code&gt;) to try to predict the &lt;code&gt;lpsa&lt;/code&gt; column (&lt;code&gt;y&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Lasso 
lasso_glmnet &amp;lt;- glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), 
                    y = prostate_train %&amp;gt;% pull(lpsa), 
                    family = &amp;quot;gaussian&amp;quot;, alpha = 1)

plot(lasso_glmnet, label = TRUE)
title(&amp;quot;Lasso Penalty&amp;quot;, line = -2.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Ridge
ridge_glmnet &amp;lt;- glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), 
                    y = prostate_train %&amp;gt;% pull(lpsa), 
                    family = &amp;quot;gaussian&amp;quot;, alpha = 0)

plot(ridge_glmnet, label = TRUE)
title(&amp;quot;Ridge Penalty&amp;quot;, line = -1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the plots show the coefficients as L1- norm increases. The top axis shows the number of nonzero coefficients correspoding to the lamba at the current L1 norm. From these plots we can see that the variables 1 (log cancer volume), 2 (log prostate weight), and 5 (seminal vesicle invasion) are good predictors for the level of prostate-specific antigen (lpsa response).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross Validation&lt;/h2&gt;
&lt;p&gt;We then want to perform cross-validation on the dataset. We use the cv.glmnet function to do this. Again we input a matrix of all of the predictors (&lt;code&gt;x&lt;/code&gt;) to look at the lpsa response (&lt;code&gt;y&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2)

# Lasso
cvglmnet_lasso &amp;lt;- cv.glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), 
                    y = prostate_train %&amp;gt;% pull(lpsa), 
                    family = &amp;quot;gaussian&amp;quot;, alpha = 0)
cvglmnet_lasso&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  cv.glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;%      as.matrix(), y = prostate_train %&amp;gt;% pull(lpsa), family = &amp;quot;gaussian&amp;quot;,      alpha = 0) 
## 
## Measure: Mean-Squared Error 
## 
##     Lambda Measure      SE Nonzero
## min 0.0879  0.5939 0.09857       8
## 1se 0.9873  0.6873 0.08098       8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(cvglmnet_lasso)
title(&amp;quot;Lasso Cross Validation&amp;quot;, line = -1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Ridge
cvglmnet_ridge &amp;lt;- cv.glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), 
                    y = prostate_train %&amp;gt;% pull(lpsa), 
                    family = &amp;quot;gaussian&amp;quot;, alpha = 1)
cvglmnet_ridge&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  cv.glmnet(x = prostate_train %&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;%      as.matrix(), y = prostate_train %&amp;gt;% pull(lpsa), family = &amp;quot;gaussian&amp;quot;,      alpha = 1) 
## 
## Measure: Mean-Squared Error 
## 
##      Lambda Measure      SE Nonzero
## min 0.01336  0.5753 0.07244       7
## 1se 0.13673  0.6408 0.09222       5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(cvglmnet_ridge)
title(&amp;quot;Ridge Cross Validation&amp;quot;, line = -1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the data output, &lt;code&gt;1se&lt;/code&gt; means the data point what is within 2 standard error of the minimum lambda (&lt;code&gt;min&lt;/code&gt;). This it the value that the model suggests that we use (indicated by the 2nd dotted line on the plots.) The &lt;code&gt;1se Measure&lt;/code&gt; is similar to the mean squared error. If the measure is small, then the model is better. When comparing the &lt;code&gt;Measure&lt;/code&gt; of the &lt;code&gt;1se&lt;/code&gt; between the two penalties, we can see that the Ridge Penalty has a smaller 1se Measure, showing that it performs better.&lt;/p&gt;
&lt;p&gt;The Nonzero column describes the nonzero coefficients, or the number of predictors that are important in the particular model. There were a total of 8 predictors as the input. The Lasso penalty shows that all 8 predictors are important in building the model, but the Ridge penalty only uses 5 predictors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lasso Prediction&lt;/h2&gt;
&lt;p&gt;As we used the training data to build the model, we can then test the generalized linear model with the lasso penalty on the testing data.&lt;/p&gt;
&lt;p&gt;We start by using the &lt;code&gt;predict&lt;/code&gt; function to use the model to predict the lpsa on the testing data. We can then see the correlation between the predicted values and actual values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s0 &amp;lt;- cvglmnet_lasso$lambda.1se 

lasso_predict &amp;lt;- predict(cvglmnet_lasso, newx = prostate_test%&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), s = s0)

# create a data frame of the actual lpsa values and the predicted lpsa values
actual_lasso_predict_df &amp;lt;- data.frame(prediction = as.vector(lasso_predict), actual = prostate_test$lpsa)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then see how correlated the prediction and real data are using the &lt;code&gt;cor&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# look at the correlation of the prediction and real data
cor(actual_lasso_predict_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            prediction    actual
## prediction  1.0000000 0.7277086
## actual      0.7277086 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output shows that the actual and predicted values are 72% correlated.&lt;/p&gt;
&lt;p&gt;We can then fit a linear line to the prediction and actual data and look at the r&lt;sup&gt;2&lt;/sup&gt; value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(actual_lasso_predict_df) %&amp;gt;%
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = actual_lasso_predict_df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.80115 -0.24539 -0.05877  0.24141  0.93424 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.40143    0.20585   6.808 2.14e-07 ***
## actual       0.42246    0.07525   5.614 5.19e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.4223 on 28 degrees of freedom
## Multiple R-squared:  0.5296, Adjusted R-squared:  0.5128 
## F-statistic: 31.52 on 1 and 28 DF,  p-value: 5.194e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The adjusted r&lt;sup&gt;2&lt;/sup&gt; value = 0.513.&lt;/p&gt;
&lt;p&gt;Finally, we can plot the actual vs. predicted values on a scatter plot. If the actual and predicted values match up exactly, they would sit on the y = x line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(actual_lasso_predict_df, aes(x = actual, y = prediction)) +
  geom_point(color = &amp;quot;#00B0F6&amp;quot;, size = 2) +
  geom_abline(slope=1, intercept=0)+
  ggtitle(&amp;quot;Lasso Prediction&amp;quot;) +
  theme_light()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ridge-prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ridge Prediction&lt;/h2&gt;
&lt;p&gt;We can then perform the same functions using the generalized linear model with the ridge penalty to test on the testing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s0 &amp;lt;- cvglmnet_ridge$lambda.1se 

ridge_predict &amp;lt;- predict(cvglmnet_ridge, newx = prostate_test%&amp;gt;% dplyr::select(lcavol:pgg45) %&amp;gt;% as.matrix(), s = s0)

# create a data frame of the predicted values and actual values
actual_ridge_predict_df &amp;lt;- data.frame(prediction = as.vector(ridge_predict), actual = prostate_test$lpsa) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then see how correlated the prediction and real data are using the &lt;code&gt;cor&lt;/code&gt; function again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(actual_ridge_predict_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            prediction    actual
## prediction  1.0000000 0.7765403
## actual      0.7765403 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ridge prediction and acutal values are 77% correlated.&lt;/p&gt;
&lt;p&gt;We can then fit a linear line to the prediction and actual data and look at the r&lt;sup&gt;2&lt;/sup&gt; value&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(actual_ridge_predict_df) %&amp;gt;%
  summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = actual_ridge_predict_df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.8438 -0.2155 -0.0225  0.2252  0.6732 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.33258    0.19271   6.915 1.62e-07 ***
## actual       0.45941    0.07044   6.522 4.55e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3953 on 28 degrees of freedom
## Multiple R-squared:  0.603,  Adjusted R-squared:  0.5888 
## F-statistic: 42.53 on 1 and 28 DF,  p-value: 4.551e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The adjusted r&lt;sup&gt;2&lt;/sup&gt; value = 0.589&lt;/p&gt;
&lt;p&gt;Finally, we can plot the actual vs. predicted values on a scatter plot. If the actual and predicted values matched up exactly, they would sit on the y = x line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(actual_ridge_predict_df, aes(x = actual, y = prediction)) +
  geom_point(color = &amp;quot;#FF62BC&amp;quot;, size = 2) +
  geom_abline(slope=1, intercept=0) +
  ggtitle(&amp;quot;Ridge Prediction&amp;quot;) +
  theme_light()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-12-exercise-solution-for-chapter-12_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Comparing the Lasso and Ridge Penalty, based on the cross-validation, the ridge penalty had a smaller 1se Measure, showing that it performs better. When looking at the actual predictions on the testing data, the ridge penalty had a higher correlation between the predicted and acutal values (77%) compared to the lasso penatly correlation (72%). Further, when fitting a linear model to the actual and predicted values, the r&lt;sup&gt;2&lt;/sup&gt; values were highter for the ridge penalty (0.589) versus the lasso penalty (0.513). In conclusion, the &lt;strong&gt;ridge penalty performed better on this particular data set.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Vocabulary for Chapter 4</title>
      <link>/post/vocabulary-for-chapter-4/</link>
      <pubDate>Thu, 20 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/vocabulary-for-chapter-4/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Chapter 4 covers how to generate both finite and infinite mixture models from various distributions. It introduces a number of terms relating to these models. The vocabulary words for Chapter 4 are:&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
finite mixture
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the context of statistic, when the distribution of interest is a combination of a few different probability distributions
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
infinite mixture
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the context of statistic, when the distribution of interest is a combination of many probability distributions (as many or more probability distributions as observations)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
mixture model
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a model for a combination of two or more different probability distributions
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
probability density function
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a function giving the relative likelihood that a continuous random variable is equal to a given value. When this function is integrated over the sample space, it equals 1.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
bimodal distribution
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a distribution comprised of two modes
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
expectation-maximization (EM) algorithm
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
an algorithm that allows for parameter estimation in probabilistic models with incomplete data
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
data augmentation
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
adding variables that are not measured (latent variables) to the data
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
latent variables
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
variables not measured in the data
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
bivariate distribution
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a combined distribution made of two random variables
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
mixture fraction
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a fraction used to describe the inhomogeneity in the mixture composition
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
identifiability
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
an issue where there can be several explanations for the same observed values; occurs when there are too many degrees of freedom in parameters
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
marginal likelihood
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the sum of the marginal distributions
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
expectation function
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a function that calculates the average of all possible values of the group that an observation belongs to
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
maximization step
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a step to optimize the parameters of a model
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
soft averaging
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the process in which observations are not assigned to groups, rather they are added to multiple groups by using probabilities of memberships as weights
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
model averaging
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the process of using several models and combining them together into a weighted model
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
zero-inflated data
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
data that contains a large number of zero counts
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
ChIP-Seq data
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
sequencing data that identifies DNA binding sites for proteins
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
chromosome
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a DNA molecule that contains the genetic material of an organism
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
binding site
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the context of molecular biology, a specific region to which a macromolecule binds
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
deoxyribonucleotide monophosphate
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a single phosphate group in a unit of DNA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
gene expression measurement
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the measurement of a functional gene product (i.e., protein or RNA)
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
microarray
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a laboratory tool used to detect gene expression
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
promoter
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the context of genetics, a region of DNA that initiates transcription of a gene
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
point mass
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a finite probabiliity concentrated at a point in the proability mass distribution at which there is a discontinuous segment in probability density function
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
sampling distribution
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the probability distribution calculated from a random sample
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
empirical cumulative distribution function (ECDF)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a step distribution function based on empirical data measurements
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
density
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
in the context of probability distributions, the derivitive of the distribution function
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
bootstrap
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
an approximation of the true sampling distribution; created by drawing new samples from the empirical distribution of the original sample
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
non-parametric method
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a statistical method that does not make assumptions about population distribution or sample size
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
nonparametric bootstrap
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
an approximation of the true sampling distribution not based off of a specific assumption or a particular model
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
Laplace distribution
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a distribution that shows differences between two independent variates with identical exponential distributions
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
gamma distribution
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a distribution that is positively valued and continuous with two parameters: shape and scale
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
negative binomial distribution/ gamma-Poisson distubtion
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the probability distribution of the number of failures before the kth success in a sequence of Bernoulli trials
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
dispersion
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the amount by which a set of observations deviate from their mean
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
variance-stabilizing transformations
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
transformations designed to give approximate independence between mean and variance
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
heteroscedasticity
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
the variance of the data is different in different regions of the data
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;border-right:1px solid;&#34;&gt;
delta method
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 30em; &#34;&gt;
a calculus procedure that uses random variables to approximate the expected value and variance of a function
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;sources-consulted-or-cited&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sources consulted or cited&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Some of the definitions above are based in part or whole on listed definitions
in the following sources.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Holmes and Huber, 2019. &lt;em&gt;Modern Statistics for Modern Biology.&lt;/em&gt; Cambridge University Press,
Cambridge, United Kingdom.&lt;/li&gt;
&lt;li&gt;Everitt and Skrondal, 2010. &lt;em&gt;The Cambridge Dictionary of Statistics (Fourth Edition).&lt;/em&gt; Cambridge University Press, Cambridge, United Kingdom.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Zero-Inflated Poisson Regression.&lt;/em&gt; Institute for Digital Research and Education Statistical Consulting. &lt;a href=&#34;https://stats.idre.ucla.edu/r/dae/zip/&#34; class=&#34;uri&#34;&gt;https://stats.idre.ucla.edu/r/dae/zip/&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Berrar, 2019. &lt;em&gt;Introduction to Non-parametric Bootstrap.&lt;/em&gt; Research Gate. &lt;a href=&#34;https://www.researchgate.net/&#34; class=&#34;uri&#34;&gt;https://www.researchgate.net/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Do and Batzoglou, 2008. &lt;em&gt;What is the expectaion maximization algorithm?.&lt;/em&gt; Nature Biotechnology.&lt;/li&gt;
&lt;li&gt;Wikipedia: The Free Encylcopedia. &lt;a href=&#34;https://en.wikipedia.org/wiki/Main_Page&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Main_Page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Google Oxford American Dictionary. &lt;a href=&#34;https://www.google.com&#34; class=&#34;uri&#34;&gt;https://www.google.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;d’Auzay, et al., 2019. &lt;em&gt;Statistics of progress variable and mixture fraction gradients in an open turbulent jet spray flame.&lt;/em&gt; Fuel.&lt;/li&gt;
&lt;li&gt;Brownlee, 2019. &lt;em&gt;A Gentle Introduction to Expectation-Maximization (EM Algorithm).&lt;/em&gt; Machine Learning Mastery. &lt;a href=&#34;https://www.machinelearningmastery.com&#34; class=&#34;uri&#34;&gt;https://www.machinelearningmastery.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Non-parametric Methods.&lt;/em&gt; R tutorial. &lt;a href=&#34;https://www.r-tutor.com&#34; class=&#34;uri&#34;&gt;https://www.r-tutor.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Precise analysis of DNA–protein binding sequences.&lt;/em&gt; Illumina. &lt;a href=&#34;https://www.illumina.com&#34; class=&#34;uri&#34;&gt;https://www.illumina.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Microarray.&lt;/em&gt; Nature. &lt;a href=&#34;https://www.nature.com&#34; class=&#34;uri&#34;&gt;https://www.nature.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;practice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Practice&lt;/h3&gt;
&lt;iframe src=&#34;https://quizlet.com/485299672/flashcards/embed?i=2r4ib&amp;amp;x=1jj1&#34; height=&#34;500&#34; width=&#34;100%&#34; style=&#34;border:0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
