Occam's Razor	Heuristic stating that the simplest explanation for a phenomenon is often the best
Rejection Region	Subset of possible null hypothesis outcomes  with probabilities falling under a low probability threshold, e.g. outcomes with a null-distribution probability < 0.05
Test Statistic	Metric for measuring how well a null hypothesis fits the data
Null Hypothesis	Hypothesis that no difference exists between certain groups of events/outcomes.
Null Distribution	Distribution of possible outcomes of an event, given that the null hypothesis is true
Rejection Region	Subset of null distribution with outcomes under a low, pre-determined probability threshold; if an outcome falls within this region (e.g. p < 0.05), it suggests that the null hypothesis is not true.
Alternative Hypothesis	A hypothesis providing a different probability distribution than the null hypothesis; conceptually, holds that some difference from the null hypothesis exists (e.g. different means, frequencies, trends)
Significance Level/False Positive Rate/Type I error	Probability of falsely rejecting the null hypothesis due to outcomes falling within the rejection region by chance; in terms of the null distribution, total probability that the outcome could fall within the rejection region given that the null hypothesis is true.
Power	True positive rate of a test (i.e. probability that an outcome falls in the rejection region of the null distribution, given that the alternative hypothesis is true)
False Negative Rate/Type II Error	Probability of falsely failing to reject the null hypothesis when an outcome from the alternative hypothesis distribution fails to fall within the rejection region of the null hypothesis.
Specificity	Complement of false positive rate; probability of a test not having a Type I error
Power/Sensitivity/True Negative Rate	complement of false negative rate (Type II error); chance of correctly rejecting null hypothesis if an outcome falls in the rejection region of the null distribution.
Assumption of independence	Treating every observation in a dataset as if it has no influence on the outcomes of other observations (or at least none unaccounted-for in the model).
P-Value Hacking	"Fallaciously ""fishing"" for significant results by running tests until a small p-value is obtained by chance; this can be a deliberate, or inadvertently caused by a scattershot approach to testing."
Hypothesis Switching	Fallacy of generating and/or changing hypotheses for a set of known results until a significant result is obtained by chance.
Family Wide Error Rate (FWER)	Probability of at least one false positive occurring in repeated tests. Assuming independent observations, this is the completement of the probability of only true positives occurring, and approaches 1.0 as the number of trials approaches infinity.
P-Value Histogram	Visualization to get a quick sense of p-value distribution of possible test outcomes for a null hypothesis. The distribution is a mixture of cases where the null hypothesis is rejected (small p-values) or retained (larger p-values).
False Discovery Rate(FDR)	The proportion of false positives among all cases where the null hypothesis is rejected across an entire distribution.
Local False Discovery Rate (fdr)	The probability of Type I Error at a given p-value when the distribution of the p-values is treated as a mixture model of the null distribution and alternative hypothesis distribution. This varies based on the p-value, rather than being a property of the entire distribution.
Tail Area False Disovery Rate (Fdr)	Integration-based extension of the local False Discovery Rate to obtain a false discovery rate for the entire distribution.
Independent Filtering	Method to increase test power by filtering variables with criteria that are independent under the null hypothesis, but correlated under the alternative (per Bourbon, Wolfgang, and Huber 2010)
Independent Hypothesis Weighting	A method of improving power of multiple testing by weighting hypotheses based on their power
Bonferroni Adjustment	Method used to compensate for inflated Type I (false positive) error in multiple testing by dividing the critical value (e.g. p = 0.05) by the number of comparisons performed
